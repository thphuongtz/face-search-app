import streamlit as st
import torch
from facenet_pytorch import MTCNN, InceptionResnetV1
from PIL import Image, ImageDraw
import numpy as np
import faiss
import os
from FAISS import build_faiss_index
import matplotlib.pyplot as plt

# --- C·∫•u h√¨nh ---
DB_PATH = "data/known_faces"
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# --- Kh·ªüi t·∫°o MTCNN v√† ResNet ---
mtcnn = MTCNN(image_size=160, margin=0, keep_all=False, post_process=True, device=device)
resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)

# --- V·∫Ω landmarks ---
def show_face_with_landmarks(img, landmarks):
    img_copy = img.copy()
    draw = ImageDraw.Draw(img_copy)
    if landmarks is not None:
        for (x, y) in landmarks.astype(int):
            r = 2
            draw.ellipse((x - r, y - r, x + r, y + r), fill=(255, 0, 0))
    return img_copy

# --- V·∫Ω ƒë·ªì th·ªã 20 vector ƒë·∫ßu ti√™n ---
def plot_embedding_comparison(query_emb, matched_emb):
    fig, ax = plt.subplots()
    x = np.arange(20)
    ax.plot(x, query_emb[:20], label='Query', marker='o')
    ax.plot(x, matched_emb[:20], label='Matched', marker='x')
    ax.set_title("So s√°nh 20 vector ƒë·∫∑c tr∆∞ng ƒë·∫ßu ti√™n")
    ax.set_xlabel("Index vector")
    ax.set_ylabel("Gi√° tr·ªã embedding")
    ax.legend()
    st.pyplot(fig)

# --- Streamlit ---
st.title("Nh·∫≠n di·ªán khu√¥n m·∫∑t - FaceNet + FAISS")

# --- T·∫£i database ---
image_files = [os.path.join(DB_PATH, f) for f in os.listdir(DB_PATH)
               if f.lower().endswith(('.jpg', '.png', '.jpeg'))]
if not image_files:
    st.error("‚ö†Ô∏è Th∆∞ m·ª•c data/known_faces tr·ªëng.")
else:
    index, filenames = build_faiss_index(image_files)
    st.success("‚úÖ Database ƒë√£ s·∫µn s√†ng!")

# --- Upload ·∫£nh ---
uploaded_file = st.file_uploader("üì∏ T·∫£i ·∫£nh ƒë·ªÉ nh·∫≠n di·ªán", type=["jpg", "png", "jpeg"])
if uploaded_file:
    img = Image.open(uploaded_file).convert("RGB")

    # --- Ph√°t hi·ªán khu√¥n m·∫∑t ---
    boxes, probs, landmarks_all = mtcnn.detect(img, landmarks=True)

    if boxes is None:
        st.error("‚ùå Kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c khu√¥n m·∫∑t!")
    else:
        # --- L·∫•y khu√¥n m·∫∑t ƒë·∫ßu ti√™n ---
        x1, y1, x2, y2 = boxes[0]
        face_pil = img.crop((x1, y1, x2, y2)).resize((160, 160))

        # --- Landmarks tr√™n crop ---
        landmarks = landmarks_all[0] if landmarks_all is not None else None
        if landmarks is not None:
            scale_x = 160 / (x2 - x1)
            scale_y = 160 / (y2 - y1)
            landmarks_crop = np.copy(landmarks)
            landmarks_crop[:, 0] = (landmarks[:, 0] - x1) * scale_x
            landmarks_crop[:, 1] = (landmarks[:, 1] - y1) * scale_y
        else:
            landmarks_crop = None

        face_landmarked = show_face_with_landmarks(face_pil, landmarks_crop)

        # --- Embedding truy v·∫•n ---
        face_tensor_resnet = torch.tensor(np.array(face_pil)).permute(2, 0, 1).float() / 255.0
        face_tensor_resnet = (face_tensor_resnet.unsqueeze(0).to(device) * 2) - 1
        query_emb = resnet(face_tensor_resnet).detach().cpu().numpy()

        # --- T√¨m khu√¥n m·∫∑t g·∫ßn nh·∫•t ---
        D, I = index.search(query_emb.astype('float32'), 1)
        matched_path = filenames[I[0][0]]
        distance = float(D[0][0])  # kho·∫£ng c√°ch FAISS (L2)
        person_name = os.path.splitext(os.path.basename(matched_path))[0]

        # --- Ng∆∞·ª°ng nh·∫≠n di·ªán ---
        threshold = 0.9  # c√≥ th·ªÉ tinh ch·ªânh 0.8‚Äì1.0 t√πy d·ªØ li·ªáu

        # --- Hi·ªÉn th·ªã ·∫£nh truy v·∫•n ---
        st.image(face_landmarked, caption="·∫¢nh truy v·∫•n (crop + landmarks, gi·ªØ m√†u g·ªëc)", width=200)

        # --- N·∫øu kh·ªõp ---
        if distance < threshold:
            # --- ·∫¢nh kh·ªõp nh·∫•t ---
            matched_img = Image.open(matched_path).convert("RGB")
            boxes_db, probs_db, landmarks_all_db = mtcnn.detect(matched_img, landmarks=True)
            if boxes_db is not None:
                x1, y1, x2, y2 = boxes_db[0]
                matched_face_pil = matched_img.crop((x1, y1, x2, y2)).resize((160, 160))
            else:
                matched_face_pil = matched_img.resize((160, 160))

            landmarks_db = landmarks_all_db[0] if landmarks_all_db is not None else None
            if landmarks_db is not None and boxes_db is not None:
                scale_x = 160 / (x2 - x1)
                scale_y = 160 / (y2 - y1)
                landmarks_crop_db = np.copy(landmarks_db)
                landmarks_crop_db[:, 0] = (landmarks_db[:, 0] - x1) * scale_x
                landmarks_crop_db[:, 1] = (landmarks_db[:, 1] - y1) * scale_y
            else:
                landmarks_crop_db = None

            matched_landmarked = show_face_with_landmarks(matched_face_pil, landmarks_crop_db)

            # --- Hi·ªÉn th·ªã song song ---
            col1, col2 = st.columns(2)
            with col1:
                st.image(face_landmarked, caption="·∫¢nh truy v·∫•n", use_container_width=True)
            with col2:
                st.image(matched_landmarked, caption=f"K·∫øt qu·∫£: {person_name}", use_container_width=True)

            # --- Th√¥ng tin nh·∫≠n d·∫°ng ---
            st.markdown("---")
            st.subheader("üìä K·∫øt qu·∫£ nh·∫≠n di·ªán")
            st.success(f"‚úÖ **Nh·∫≠n di·ªán th√†nh c√¥ng:** {person_name}\n\nüìè Kho·∫£ng c√°ch: `{distance:.4f}` (C√πng ng∆∞·ªùi)")

            # --- So s√°nh vector ---
            matched_tensor_resnet = torch.tensor(np.array(matched_face_pil)).permute(2, 0, 1).float() / 255.0
            matched_tensor_resnet = (matched_tensor_resnet.unsqueeze(0).to(device) * 2) - 1
            matched_emb = resnet(matched_tensor_resnet).detach().cpu().numpy()

            plot_embedding_comparison(query_emb.flatten(), matched_emb.flatten())

        # --- N·∫øu KH√îNG kh·ªõp ---
        else:
            st.markdown("---")
            st.subheader("üìä K·∫øt qu·∫£ nh·∫≠n di·ªán")
            st.warning(f"‚ö†Ô∏è Kh√¥ng t·ªìn t·∫°i trong d·ªØ li·ªáu!\n\nüìè Kho·∫£ng c√°ch g·∫ßn nh·∫•t: `{distance:.4f}`")
